# chatgpt-vs-claude-prompt-testing
A comparative analysis of ChatGPT and Claude 3 on reasoning, summarization, and logic prompts.

# ğŸ” Prompt Quality Testing: ChatGPT vs Claude

This project evaluates prompt responses across 5 core task types to assess output quality between **OpenAIâ€™s ChatGPT** and **Anthropicâ€™s Claude**.

---

## âœ… Test Categories

1. **Reasoning**  
2. **Summarization**  
3. **Information Extraction**  
4. **Formatting**  
5. **Logical Flow**

Each prompt was run on both models. Responses were analyzed and scored across:
- **Accuracy**
- **Clarity**
- **Structure**
- **Edge Case Handling**

---

## ğŸ“Š Final Verdict

| Task Type      | Claude Score | ChatGPT Score | Winner   |
|----------------|--------------|----------------|----------|
| Reasoning      | 10/10        | 10/10          | Tie      |
| Summarization  | 10/10        | 10/10          | Tie      |
| Extraction     | 10/10        | 10/10          | Tie      |
| Formatting     | 10/10        | 10/10          | Tie      |
| Logic          | 10/10        | 10/10          | Tie      |

Both models performed at the highest level of consistency and correctness for this test suite.

---

## ğŸ“ Folder Structure

```bash
â”œâ”€â”€ prompts/
â”‚   â”œâ”€â”€ 01_reasoning.md
â”‚   â”œâ”€â”€ 02_summarization.md
â”‚   â”œâ”€â”€ 03_extraction.md
â”‚   â”œâ”€â”€ 04_formatting.md
â”‚   â””â”€â”€ 05_logic.md
â”œâ”€â”€ responses/
â”‚   â”œâ”€â”€ chatgpt/
â”‚   â”‚   â””â”€â”€ reasoning.txt (etc.)
â”‚   â””â”€â”€ claude/
â”‚       â””â”€â”€ reasoning.txt (etc.)
â”œâ”€â”€ analysis/
â”‚   â””â”€â”€ comparison-notes.md
â”œâ”€â”€ README.md

